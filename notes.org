* Background
Recent Survey Paper:
From Statistical Relational to Neuro-Symbolic Artificial Intelligence
https://arxiv.org/pdf/2003.08316.pdf

Identify a 7 dimensional space of Learning Combined with reasoning for
AI.

1. Directed vs undirected graphs
2. Grounding vs proof
3. Integrating Logic with Probability
4. Logical semantics
5. Learning Parameters vs structure
6. Representing entities with symbols or sub-symbols
7. The type of Logic used.

My initial thoughts are that the important areas to think about in the
context of thinking about MR from an AI perspective is that dimensions
1,3 and 5 are clearly relevant to the topic at hand.

** MR as a directed graph (1).

MR is often explained using directed acyclic graphs.
The types of Statistical and relational artificial intelligence (STAR-AI) that generalizes directed graphs, include  plate notation
, probabilistic relational models (PRMs), probabilistic logic programs (PLPs), and Bayesian
logic programs. The most typical and popular are probabilistic
logic programs. (Refs in the survey paper). Undirected graphs on the
other hand have been explored with Markov
networks and random fields. 

A difference (between directed vs undirected) from a logical
perspective is the difference between using definite clauses i.e. prolog
or full horn logic. (Multiple disjunctive heads to a clause).

First type of rule (definite), shows what can be inferred from
what. (The implication sign being a kind of cause). The second type of
rule are a form of soft constraints. (Maybe relevant to Katsumis work
on disjunctive abduction
https://link.springer.com/article/10.1007%2Fs00354-019-00059-x)

In the context of refining MR methodology, potentially both types of
rule might be useful, i.e. General inferences on implication but also
inference of constraints on parameters and structures.

** Logic and probability (3)
Semantics of boolean logic vs continues values.
Fuzzy logic to deal with vagueness.

MR parameters are probabilities.

** Learning Parameters vs structure (5)

I think learning parameters is like estimating beta values.

Learning structure ..

Abduction of non-measured confounding elements.

* Abduction vs induction.

The life sciences unlike physics are often concerned with learning
explanations rather than general rules. So abduction is often a good
way to think about problems rather than induction. 

But learning MR might be considered induction as we are trying to
learn general rules.

Meta abduction is abduction at the meta level i.e. imperial work.

* Kant

Richard Evans work makes kantian assumptions:

1. 
2. 
3. 

* Reinforcement learning to learn the value function.

A genetic algorithm can be thought of as a reinforcement learning
algorithm, there is an exploration and exploitation trade off. 
Genetic algorithm is a randomized beam search.
Random search is useful when the space is large.
Evolution is a randomized beam search.
The search is to build a replicating agent.
All genes are under selection.
An assumption of MR is that there is random mating and that they are
not under selection.

Studies have shown that the distribution of genetic variants is uniform
across the population (European)[ Davey Smith, 2011 - Use of genetic
markers and gene-diet interactions for interrogating population level
causal influences of diet on health ]

The genetic book of the dead. Reading the genome tells you about the
enviorments that the ancestors of the organism lived in.

How does evolution work with out sex. Is the search only local? 

Evolution is MR - Gib.

The genome causes the phenome. (Both are structured objects)

A phenotype is a manifestation of the genome that can be selected
upon.

The genome interacts with the enviorment to create the phenotype.

Selection is non-random.
* Exceptional Model Mining and RSD
** Linear regression 
Phenotype = a + beta_Snp +e

If we look at a subgroup of people:

Phenotype = a_group + beta_group_snp + e

Is the beta value differnt?

The phenotype and SNP are fixed and we search for the subgroup. Where
beta and beta complement are differnt.

Thresholds on principle components.
*** Causal models stratified analysis.
Maybe stratify on frequent queries?

Think about: https://advances.sciencemag.org/content/6/16/eaay0328

* Kernel PCA for population structures (Masters project?)
** Use a structure kernel to calculate distance between pairs of haplotypes then do kernel PCA compare to non kernel pca.
** Also WARMR type approach to find population sub structures on structured data.
*** Could this do something with family structures and things like assortivitve mating?
